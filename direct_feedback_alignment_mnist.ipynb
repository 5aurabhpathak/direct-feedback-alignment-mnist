{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "expermients.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15N2V9kd7HkretK6BVQn49kngoGT66PUX",
      "authorship_tag": "ABX9TyN73qlyY+wH+q8HnTEcZ/9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/5aurabhpathak/neural-net-training-algorithms/blob/main/direct_feedback_alignment_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o43UZgvyRLKb"
      },
      "source": [
        "! pip install -U tensorboard_plugin_profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U4ctnaWKnna"
      },
      "source": [
        "import numpy as np, os, tensorflow as tf, tensorflow.keras as keras, pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.layers import Dense, Input, ReLU, Softmax\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras import Model\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi817-ui6Et6"
      },
      "source": [
        "mnist_train = pd.read_csv('sample_data/mnist_train_small.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK5U66D17VKe"
      },
      "source": [
        "X, Y = mnist_train.iloc[:,1:].to_numpy(), mnist_train.iloc[:,:1].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd8BrqwgTFEy"
      },
      "source": [
        "x = X / 255.\n",
        "y = keras.utils.to_categorical(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am_AbYzZqXzs"
      },
      "source": [
        "class DFAModel(Model):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    for i, layer in enumerate(self.layers[1:-1]):\n",
        "      fanout = self.layers[i+1].output_shape[1]\n",
        "      layer.feedback = tf.random.uniform(minval=-1./fanout**.5,\n",
        "                                         maxval=1./fanout**.5,\n",
        "                                         shape=(self.output.shape.as_list()[1],\n",
        "                                                layer.output_shape[1]))\n",
        "    self._name = 'model_dfa'\n",
        "\n",
        "  def train_step(self, data):\n",
        "    x, y = data\n",
        "\n",
        "    outs = []\n",
        "    with tf.GradientTape(watch_accessed_variables=False, persistent=True) as tape:\n",
        "      tape.watch(self.trainable_variables)\n",
        "      for layer in self.layers[1:-1]:\n",
        "        x = layer(tf.stop_gradient(x), training=True)\n",
        "        yl = tf.matmul(y, layer.feedback)\n",
        "        outs.append(x)\n",
        "      yp = self.layers[-1](x)\n",
        "      tape.watch(yp)\n",
        "      loss = self.compiled_loss(y, yp, regularization_losses=self.losses)\n",
        "\n",
        "    dl_dy = tf.reduce_sum(tape.gradient(loss, yp), axis=0, keepdims=True)\n",
        "    # print('dl_dy:', dl_dy.shape)\n",
        " \n",
        "    gradients = []\n",
        "    for i, layer in enumerate(self.layers[1:-1]):\n",
        "      with tf.name_scope(f'{layer.name}/local_grads'):\n",
        "        da_dw, da_db = tape.gradient(outs[i], [layer.kernel, layer.bias])\n",
        "        # print(layer.name, ':\\nda_dw:', da_dw.shape, 'da_db:', da_db.shape)\n",
        "\n",
        "      with tf.name_scope(f'{layer.name}/global_feedback'):\n",
        "        dl_da = tf.reduce_sum(tape.gradient(loss, outs[i]), axis=0, keepdims=True)\n",
        "        dl_da = tf.matmul(dl_dy, layer.feedback)\n",
        "        # print('dl_da:', dl_da.shape)\n",
        "\n",
        "      with tf.name_scope(f'{layer.name}/updates'):\n",
        "        dl_dw = tf.multiply(dl_da, da_dw)\n",
        "        dl_db = tf.squeeze(tf.multiply(dl_da, da_db))\n",
        "        # print('dl_dw:', dl_dw.shape, 'dl_db:', dl_db.shape)\n",
        "      gradients.extend([dl_dw, dl_db])\n",
        "\n",
        "      # dl_dw1, dl_db1 = tape.gradient(loss, [layer.kernel, layer.bias])\n",
        "      # tf.debugging.assert_near(dl_dw, dl_dw1)\n",
        "      # tf.debugging.assert_near(dl_db, dl_db1)\n",
        "\n",
        "    dl_dw, dl_db = tape.gradient(loss, [self.layers[-1].kernel,\n",
        "                                        self.layers[-1].bias])\n",
        "    # print(self.layers[-1].name, ':\\ndl_dw:', dl_dw.shape, 'dl_db:', dl_db.shape)\n",
        "    gradients.extend([dl_dw, dl_db])\n",
        "\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "    self.compiled_metrics.update_state(y, yp)\n",
        "\n",
        "    return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQerDmV6bfpe"
      },
      "source": [
        "LayerConfig = namedtuple('LayerConfig', 'cls args kwargs')\n",
        "\n",
        "def get_twin_models(layers):\n",
        "\n",
        "  def get_model(tag):\n",
        "    layer_name = layers[0].kwargs['name']\n",
        "    kwargs = {x:y for x, y in layers[0].kwargs.items() if x != 'name'}\n",
        "    inp = layers[0].cls(*layers[0].args, name=f'{layer_name}_{tag}', **kwargs)\n",
        "    x = inp\n",
        "    for layer in layers[1:]:\n",
        "      layer_name = layer.kwargs['name']\n",
        "      if tag == 'bp':\n",
        "        kwargs = {x:y for x, y in layer.kwargs.items()\n",
        "                  if x not in {'name', 'kernel_initializer'}}\n",
        "      else:\n",
        "        kwargs = {x:y for x, y in layer.kwargs.items() if x != 'name'}\n",
        "      x = layer.cls(*layer.args, name=f'{layer_name}_{tag}', **kwargs)(x)\n",
        "    \n",
        "    if tag == 'dfa':\n",
        "      model = DFAModel(inputs=inp, outputs=x)\n",
        "    else:\n",
        "      model = Model(inputs=inp, outputs=x, name='model_bp')\n",
        "    model.summary()\n",
        "    weights = model.get_weights()\n",
        "    return model, weights\n",
        "\n",
        "  return get_model\n",
        "\n",
        "\n",
        "layers = [\n",
        "          LayerConfig(Input, (784,), dict(name='Input1')),\n",
        "          LayerConfig(Dense, (800,), dict(activation='tanh',\n",
        "                                          kernel_initializer='zeros',\n",
        "                                          name='Dense1')),\n",
        "          LayerConfig(Dense, (800,), dict(activation='tanh',\n",
        "                                          kernel_initializer='zeros',\n",
        "                                          name='Dense2')),\n",
        "          LayerConfig(Dense, (10,), dict(kernel_initializer='zeros',\n",
        "                                         name='Dense3'))\n",
        "        ]\n",
        "\n",
        "get_model = get_twin_models(layers)\n",
        "model_dfa, weights_dfa = get_model('dfa')\n",
        "model_bp, weights_bp = get_model('bp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxGwPw0R6fqY"
      },
      "source": [
        "# log dir\n",
        "save_path = os.path.join('logs')\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "! rm -rf $save_path/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWrdeWlFps_N"
      },
      "source": [
        "! sudo pkill -9 tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqNES9ek6xxW"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir $save_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWptIY6IwGsF"
      },
      "source": [
        "! cp -r logs /content/drive/MyDrive/experiments/logs1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNav58a267uj"
      },
      "source": [
        "def train_twins(x, y, model_bp, model_dfa, weights_bp, weights_dfa):\n",
        "  def train(tag):\n",
        "    log_dir = os.path.join(save_path, tag)\n",
        "    # filepath = os.path.join(log_dir, 'model.{epoch:02d}-{val_loss:.4f}.h5')\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    callbacks = [\n",
        "        # EarlyStopping(patience=10, verbose=1),\n",
        "        # ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.00001, verbose=1),\n",
        "        # ModelCheckpoint(filepath=filepath, verbose=1,\n",
        "        #                 save_best_only=True, save_weights_only=True),\n",
        "        TensorBoard(log_dir=log_dir,\n",
        "                    histogram_freq=1,\n",
        "                    write_graph=True,\n",
        "                    write_images=False,\n",
        "                    write_steps_per_second=True,\n",
        "                    update_freq='batch',\n",
        "                    profile_batch=0,\n",
        "                    embeddings_freq=0,\n",
        "                    embeddings_metadata=None)]\n",
        "\n",
        "    if tag == 'dfa':\n",
        "      model, weights = model_dfa, weights_dfa\n",
        "    else:\n",
        "      model, weights = model_bp, weights_bp\n",
        "\n",
        "    model.set_weights(weights)\n",
        "    model.compile(\n",
        "        # run_eagerly=True,\n",
        "        optimizer=RMSprop(learning_rate=.002), loss=CategoricalCrossentropy(from_logits=True))\n",
        "    return model.fit(x, y,\n",
        "              epochs=50,\n",
        "              batch_size=32,\n",
        "              validation_split=.1,\n",
        "              callbacks=callbacks)\n",
        "  \n",
        "  return train\n",
        "\n",
        "train_fn = train_twins(x, y, model_bp, model_dfa, weights_bp, weights_dfa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEG6hSBg5ZQ5"
      },
      "source": [
        "! rm -rf $save_path/dfa/*\n",
        "res_dfa = train_fn('dfa')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_ZXudbD3Q9Z"
      },
      "source": [
        "! rm -rf $save_path/bp/*\n",
        "res_bp = train_fn('bp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj8NICtF8cpb"
      },
      "source": [
        "def plot_metric(res, metric, mark_epoch, *, logy=False):\n",
        "  val_metric = f'val_{metric}'\n",
        "  plt.title(metric)\n",
        "  func = plt.plot if not logy else plt.semilogy\n",
        "  ylabel = metric if not logy else f'log_{metric}'\n",
        "  func(res.history[metric], label=metric)\n",
        "  func(res.history[val_metric], label=val_metric)\n",
        "  plt.scatter(mark_epoch,\n",
        "              res.history[val_metric][mark_epoch],\n",
        "              marker='^', color='r', s=50, label='best model')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel(ylabel)\n",
        "\n",
        "best_epoch = np.argmin(res_dfa.history['val_loss'])\n",
        "n_metrics = len(model_dfa.metrics_names)\n",
        "nr, nc = n_metrics//3 + (n_metrics%3 != 0), 3\n",
        "fig, axes = plt.subplots(nr, nc, sharex=True, figsize=(15, 8))\n",
        "for i, metric in enumerate(model_dfa.metrics_names):\n",
        "  plt.subplot(nr, nc, i+1)\n",
        "  plot_metric(res_dfa, metric, best_epoch, logy=True)\n",
        "\n",
        "rem = (nr * 3) - n_metrics\n",
        "[fig.delaxes(ax) for ax in axes.ravel()[-rem:]]\n",
        "plt.tight_layout()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dwxf4DIBaxY"
      },
      "source": [
        "best_weights = f'model.{best_epoch+1:02d}-{res_dfa.history[\"val_loss\"][best_epoch]:.4f}.h5'\n",
        "model_dfa.load_weights(os.path.join(save_path, 'dfa', best_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9LQuS4zDjzZ"
      },
      "source": [
        "# yp_bp = model_bp.predict(x)\n",
        "yp_dfa = model_dfa.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRbDyS24EFep"
      },
      "source": [
        "# plt.scatter(x,yp_bp)\n",
        "plt.scatter(x,yp_dfa)\n",
        "plt.scatter(x,y, alpha=.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNqximkmQ0eT"
      },
      "source": [
        "x = np.random.uniform(size=(5000,1)).astype('float32')\n",
        "y = np.sin(2.*np.pi*x) + np.random.uniform(-.1, .1, size=x.shape)\n",
        "plt.scatter(x,y)\n",
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVafY3H46Z9i"
      },
      "source": [
        "def fit(model, x, y, epochs=200, lr=.001, mode='bp'):\n",
        "  xs, weights, grads, ys, cost = [], [], [], [], []\n",
        "\n",
        "  if mode == 'dfa':\n",
        "      for i, layer in enumerate(model.layers[:-1]):\n",
        "        if 'Input' in layer.name:\n",
        "          continue\n",
        "        layer.feedback = tf.random.uniform(minval=-1., maxval=1., shape=(layer.output_shape[1], model.output[-1].shape.as_list()[1]))\n",
        "      model.layers[-1].feedback = tf.eye(model.output[-1].shape.as_list()[1])\n",
        "\n",
        "  for j in range(epochs):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      all_out = model(x)\n",
        "      yp = all_out[-1]\n",
        "      # print(all_out[0], all_out[1])\n",
        "      loss = .5 * tf.reduce_mean((yp - y) ** 2.)\n",
        "\n",
        "    if mode == 'dfa':\n",
        "      dl_dy = tape.gradient(loss, yp)\n",
        "      # print('global error:', dl_dy)\n",
        " \n",
        "    # print(loss)\n",
        "    for i, layer in enumerate(model.layers):\n",
        "      if 'Input' in layer.name:\n",
        "        continue\n",
        "      # print(layer.weights)\n",
        "      if mode == 'bp':\n",
        "        dl_da = tape.gradient(loss, all_out[i-1])\n",
        "        dl_dw1, dl_db1 = tape.gradient(loss, [layer.kernel, layer.bias])\n",
        "        da_dw, da_db = tape.jacobian(all_out[i-1], [layer.kernel, layer.bias])\n",
        "        # print('layer_error:', dl_da, da_dw, da_db)\n",
        "        dl_dw = tf.reduce_sum(tf.concat([x * y for x, y in zip(tf.unstack(dl_da, axis=0), tf.unstack(tf.reduce_sum(da_dw, axis=1), axis=0))], axis=0), axis=0, keepdims=True)\n",
        "        dl_db = tf.reduce_sum(tf.concat([x * y for x, y in zip(tf.unstack(dl_da, axis=0), tf.unstack(tf.reduce_sum(da_db, axis=1), axis=0))], axis=0), axis=0, keepdims=True)\n",
        "        dl_dw1, dl_db1 = tape.gradient(loss, [layer.kernel, layer.bias])\n",
        "        tf.debugging.assert_near(dl_dw, dl_dw1)\n",
        "        tf.debugging.assert_near(dl_db, dl_db1)\n",
        "      else:\n",
        "        layer_out = all_out[i-1]\n",
        "        da_dw, da_db = tape.jacobian(layer_out, [layer.kernel, layer.bias])\n",
        "        # print('feedback', layer.feedback)\n",
        "        dl_da = dl_dy * layer.feedback\n",
        "        # print('layer_error:', dl_da)\n",
        "        dl_dw = tf.reduce_sum(tf.concat([x * y for x, y in zip(tf.unstack(dl_da, axis=0), tf.unstack(tf.reduce_sum(da_dw, axis=1), axis=0))], axis=0), axis=0, keepdims=True)\n",
        "        dl_db = tf.reduce_sum(tf.concat([x * y for x, y in zip(tf.unstack(dl_da, axis=0), tf.unstack(tf.reduce_sum(da_db, axis=1), axis=0))], axis=0), axis=0, keepdims=True)\n",
        "        # print(dl_dw, dl_db)\n",
        "        # print(layer_out.shape, da_dw.shape, da_db.shape, dl_da.shape, dl_dw.shape, dl_db.shape)\n",
        " \n",
        "      layer.kernel.assign(tf.subtract(layer.kernel, tf.multiply(tf.constant(lr), dl_dw)))\n",
        "      layer.bias.assign(tf.subtract(layer.bias, tf.multiply(tf.constant(lr), dl_db)))\n",
        "    ys.append(yp)\n",
        "    cost.append(loss)\n",
        "    print('\\r', j+1, end='')\n",
        "  return ys, cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qntubf4w74pF"
      },
      "source": [
        "model.set_weights(weights)\n",
        "ys, cost = fit(model, x, y, 500, 1., 'dfa')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FeCrYVr1dZp"
      },
      "source": [
        "numpy.abs(model.predict(x)[-1] - y).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGYmXvfq99ya"
      },
      "source": [
        "ys = numpy.asarray([yy.numpy() for yy in ys])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI9AZsDE-XoX"
      },
      "source": [
        "cost = numpy.asarray([yy.numpy() for yy in cost])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7htU6P34XzvI"
      },
      "source": [
        "# plt.subplot(221)\n",
        "# plt.plot(grads)\n",
        " \n",
        "# plt.subplot(222)\n",
        "# plt.plot(weights)\n",
        " \n",
        "plt.subplot(223)\n",
        "plt.plot(cost)\n",
        " \n",
        "plt.subplot(224)\n",
        "# plt.plot(ys)\n",
        "plt.plot(ys[:,-1])\n",
        "# plt.scatter(xs[100:], ys[100:,0,:])\n",
        "# plt.scatter(xs[100:], ys[100:,1,:])\n",
        "ys[-5:,-1], y[-1]#, weights[-5:], grads[-5:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF5Ifpspj68P"
      },
      "source": [
        "xx = numpy.argsort(x.ravel())\n",
        "plt.plot(x[xx], model.predict(x)[-1][xx])\n",
        "plt.plot(x[xx], y[xx])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}